{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron: application to the Iris dataset\n",
    "## Luiz Fernando Rodrigues\n",
    "\n",
    "Here I wanted to play around with the Iris dataser from sklearn. The idea is to perform a pairwise classification using perceptrons, and then joining everything in the end, with not unanimously classified point being set randomly. We evaluate $E_{in}$ according to Prof. Dr. Abu-Mostafa's course (\"Learning from data\").\n",
    "\n",
    "### OBS: some variable names are in Portuguese"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lufe/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 3\n",
    "Este exercício trabalha com o dataset *Iris* do *sklearn*. O dataset apresenta 4 características com 3 diferentes classes, e um total de 150 pontos (50 por classe). O exercício é:\n",
    "## Item a) \n",
    "Fazer um PLA do tipo *pocket* com critério de parada;\n",
    "## Item b) \n",
    "Rodar o PLA para cada par de classes separadamente, avaliando $E_{in}$;\n",
    "## Item c) \n",
    "Criar um novo classificador $g(\\vec{x})$ que tenha como saída a moda dos três classificadores do item b) (caso seja única), ou um valor aleatório entre $\\lbrace 0,1,2 \\rbrace$ caso contrário. Como há um fator de aleatoriedade, rode 1000 experimentos para estimar $E_{in}$, comparando o resultado com o do item b).\n",
    "\n",
    "### Os itens estão respondidos em conjunto abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data and target features in variables\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Changing numpy's sign function, so 0 has output of 1\n",
    "def sinal(arg):\n",
    "    if np.sign(arg) == 1:\n",
    "        return 1\n",
    "    if np.sign(arg) == 0:\n",
    "        return 1\n",
    "    if np.sign(arg) == -1:\n",
    "        return -1\n",
    "    \n",
    "# Creates a list of misclassified points\n",
    "def wrong(x_set, y_set, w_handle):\n",
    "    misclassified = []\n",
    "    for j in range(0,100):\n",
    "        if sinal(np.inner(w_handle,x_set[j])) != y_set[j]:\n",
    "            misclassified.append(j)\n",
    "    return misclassified\n",
    "\n",
    "# Inserts the dummy variable x_0 = 1 so we can implement the perceptron\n",
    "x = np.insert(x, 0, 0, axis = 1)\n",
    "\n",
    "# We will use a 2D array to store the hypotheses of each perceptron procedure (since we will compare 0 to 1, 0 to 2 and 1 to 2)\n",
    "w = np.zeros((3,np.size(x,axis=1)))\n",
    "# We will also have an array of e_in values\n",
    "e_in = np.zeros(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our perceptrons will be of a _pocket_ style, preserving the best result throughout the procedure, and setting a limit on the number of iterations so we do not fall into endless loops in case of misclassified points.\n",
    "\n",
    "### Defining our perceptron ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x_set, y_set):\n",
    "    k = 0 #iteration counter\n",
    "    w_handle = np.zeros(np.size(x,axis=1)) #auxiliary variable\n",
    "    misclassified_indexes = wrong(x_set, y_set, w_handle)\n",
    "    # Initializing the number of wrong guesses, to assess whether I keep a w in the pocket or not\n",
    "    n_misclassified = len(misclassified_indexes)\n",
    "    while len(misclassified_indexes) > 0:\n",
    "        index = np.random.choice(misclassified_indexes) # Choose a wrongly classified point randomly\n",
    "        w_handle = w_handle + y_set[index]*x_set[index]\n",
    "        k = k+1\n",
    "        misclassified_indexes = wrong(x_set, y_set, w_handle)\n",
    "        if len(misclassified_indexes) <= n_misclassified: # If we improve the result, update the pocket\n",
    "            w_pocket = w_handle\n",
    "            n_misclassified = len(misclassified_indexes) # Updates\n",
    "        #Critério de parada: parar após 1000 iterações \n",
    "        if k > 1000:\n",
    "            print('Stopped: limit of 1000 iterations.')\n",
    "            break\n",
    "    e_in_pocket = n_misclassified/100\n",
    "    return w_pocket, e_in_pocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron 1: classes 0 and 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The w vector for y = 0 and y = 1 is:\n",
      "[ 0.  -1.5 -6.   7.6  3.7]\n",
      "\n",
      "\n",
      "E_in (0-1) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Selects data of y being either 0 or 1, and converts 0 -> -1 so we can use the perceptron\n",
    "y_set01 = y[y != 2]\n",
    "x_set01 = x[:-50]\n",
    "\n",
    "for i in range(0, len(y_set01)):\n",
    "    if y_set01[i] == 0:\n",
    "        y_set01[i] = -1\n",
    "\n",
    "# The dataset is already ordered, so we do not need to filter anything\n",
    "\n",
    "w[0], e_in[0] = perceptron(x_set01, y_set01)\n",
    "print('The w vector for y = 0 and y = 1 is:')\n",
    "print(w[0])\n",
    "print('\\n')\n",
    "print('E_in (0-1) = ' + str(e_in[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron 2: classes 0 and 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The w vector for y = 0 and y = 2 is:\n",
      "[ 0.  -2.5 -4.2  5.8  2.9]\n",
      "\n",
      "\n",
      "E_in (0-2) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Selects data of y being either 0 or 1, and converts 0 -> -1 so we can use the perceptron\n",
    "y_set02 = y[y != 1]\n",
    "x_set02 = np.zeros((100,5))\n",
    "\n",
    "#Loop para tirar os termos do \"meio\" do dataset, de classe 1\n",
    "for j in range(0,50):\n",
    "    x_set02[j] = x[j]\n",
    "    x_set02[j+50] = x[j+100]\n",
    "\n",
    "for i in range(0,len(y_set02)):\n",
    "    if y_set02[i] == 0:\n",
    "        y_set02[i] = -1\n",
    "    if y_set02[i] == 2:\n",
    "        y_set02[i] = 1\n",
    "\n",
    "# The dataset is already ordered, so we do not need to filter anything\n",
    "\n",
    "w[1], e_in[1] = perceptron(x_set02, y_set02)\n",
    "print('The w vector for y = 0 and y = 2 is:')\n",
    "print(w[1])\n",
    "print('\\n')\n",
    "print('E_in (0-2) = ' + str(e_in[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron 3: classes 1 and 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped: limit of 1000 iterations.\n",
      "The w vector for y = 1 and y = 2 is:\n",
      "[  0.  -67.7 -69.7  91.5 107.5]\n",
      "\n",
      "\n",
      "E_in (1-2) = 0.03\n"
     ]
    }
   ],
   "source": [
    "#Seleciona os dados de com y sendo 1 ou 2, e converte 1 -> -1 e 2 -> 1 para poder usar o PLA\n",
    "y_set12 = y[y != 0]\n",
    "x_set12 = x[50:]\n",
    "\n",
    "for i in range(0, len(y_set12)):\n",
    "    if y_set12[i] == 1:\n",
    "        y_set12[i] = -1\n",
    "    if y_set12[i] == 2:\n",
    "        y_set12[i] = 1\n",
    "        \n",
    "# The dataset is already ordered, so we do not need to filter anything\n",
    "\n",
    "w[2], e_in[2] = perceptron(x_set12, y_set12)\n",
    "print('The w vector for y = 1 and y = 2 is:')\n",
    "print(w[2])\n",
    "print('\\n')\n",
    "print('E_in (1-2) = ' + str(e_in[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Since for the two first perceptrons we had no misclassified points, we can infer that the data is linearly separable between 0 and 1, and 0 and 2. Things get more complicated between 1 and 2, but we can safely say the the data of class 0 is more clearly defined for this feature space. Also, we have only $E_{in,1-2} = 3\\%$, so it is also not the end of the world. We can repeat the process to check whether the misclassified points are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified point in repetition 0\n",
      "[0.  5.9 3.2 4.8 1.8]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 0\n",
      "[0.  6.3 2.5 4.9 1.5]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 0\n",
      "[0.  6.  2.7 5.1 1.6]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 500\n",
      "[0.  5.9 3.2 4.8 1.8]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 500\n",
      "[0.  6.3 2.5 4.9 1.5]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 500\n",
      "[0.  6.  2.7 5.1 1.6]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 999\n",
      "[0.  5.9 3.2 4.8 1.8]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 999\n",
      "[0.  6.3 2.5 4.9 1.5]\n",
      "1\n",
      "\n",
      "Misclassified point in repetition 999\n",
      "[0.  6.  2.7 5.1 1.6]\n",
      "1\n",
      "\n",
      "Mean of misclassified points: 3.0\n",
      "Standard deviation: 0.0\n",
      "Unique values: [3.]\n",
      "Unique counts: [1000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We will create a class array for each classifier (perceptron for each class pair)\n",
    "# Class pairs: 0,1 ; 0,2 ; 1,2\n",
    "y01 = np.zeros(150)\n",
    "y02 = np.zeros(150)\n",
    "y12 = np.zeros(150)\n",
    "\n",
    "# Calculates the classification and maps -1 and 1, according to each case, to 0, 1 or 2\n",
    "for i in range(0,150):\n",
    "    y01[i] = sinal(np.inner(w[0],x[i]))\n",
    "    if y01[i] == -1:\n",
    "        y01[i] = 0\n",
    "    elif y01[i] == 1:\n",
    "        y01[i] = 1\n",
    "    y02[i] = sinal(np.inner(w[1],x[i]))\n",
    "    if y02[i] == -1:\n",
    "        y02[i] = 0\n",
    "    elif y02[i] == 1:\n",
    "        y02[i] = 2\n",
    "    y12[i] = sinal(np.inner(w[2],x[i]))\n",
    "    if y12[i] == -1:\n",
    "        y12[i] = 1\n",
    "    elif y12[i] == 1:\n",
    "        y12[i] = 2\n",
    "\n",
    "# Calculates the new classifier, finds E_in and repeats the process 1000 times\n",
    "e_in_multiple = np.zeros(1000)\n",
    "for i in range(0,1000):\n",
    "    g = np.zeros(150) # New classification vector\n",
    "    for j in range(0,150):\n",
    "        # Finds the mode between the classifiers\n",
    "        if (y01[j] == y02[j]):\n",
    "            g[j] = y01[j]\n",
    "        elif (y01[j] == y12[j]):\n",
    "            g[j] = y12[j]\n",
    "        elif (y02[j] == y12[j]):\n",
    "            g[j] = y02[j]\n",
    "        # If there is no mode, it just randomly picks a class\n",
    "        else:\n",
    "            g[j] = int(3*np.random.random_sample())\n",
    "            #print('Random pick for point ' + str(j) + ' in repeat number ' + str(i)) # Tells when it had to pick randomly\n",
    "        if (g[j] != y[j]):\n",
    "            e_in_multiple[i] = e_in_multiple[i] + 1\n",
    "            # Out of curiosity, let's verify which points are wrong to see if those are the same from the perceptron (case 1-2)\n",
    "            # And we will do it for three repeats (the first, the last and the middle one), to check if it is the same\n",
    "            if i == 0 or i == 500 or i == 999:\n",
    "                print('Misclassified point in repetition ' + str(i))\n",
    "                print(x[j])\n",
    "                print(y[j])\n",
    "                print('')\n",
    "\n",
    "print('Mean of misclassified points: ' + str(np.mean(e_in_multiple)))\n",
    "print('Standard deviation: ' + str(np.std(e_in_multiple)))\n",
    "unique_e_in = np.unique(e_in_multiple, return_counts=True)\n",
    "print('Unique values: ' + str(unique_e_in[0]))\n",
    "print('Unique counts: ' + str(unique_e_in[1]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "As we can see above, two to three points were misclassified by our sequential perceptron approach, and basically the same three points. Considering how simple our approach was on classifying, this 2% error (3/150) indicates that the data is mostly linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
